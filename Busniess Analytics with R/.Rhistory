res.sum$which[3,]
#library(caTools)
set.seed(42)
sample = sample.split(hitters.na.out$log_salary, SplitRatio = .80)
train.df= subset(hitters.na.out, sample == TRUE)
valid.df= subset(hitters.na.out, sample == FALSE)
#library(MASS),library(rpart),library(rpart.plot)
tree.baseball <- rpart(log_salary ~ Hits + Years,data =train.df)
#summary(tree.baseball)
rpart.plot(tree.baseball)
tree.baseball$variable.importance
tree.baseball_1<-rpart(log_salary~.,data=train.df)
#summary(tree.baseball_1)
rpart.plot(tree.baseball_1)
tree.baseball_1$variable.importance
lambdas <- c(c(), seq(0.002, 0.01, by=0.001))
lambdas <- c(lambdas, seq(0.02, 0.1, by=0.01))
lambdas <- c(lambdas, seq(0.2, 1, by=0.1))
length.lambdas <- length(lambdas)
train_errors <- rep(NA, length.lambdas)
test_errors <- rep(NA, length.lambdas)
for (i in 1:length.lambdas) {
boost.hitters <- gbm(log_salary~ ., data = train.df, distribution = "gaussian",
n.trees = 1000, shrinkage = lambdas[i])
train_pred <- predict(boost.hitters, train.df, n.trees = 1000)
test_pred <- predict(boost.hitters, valid.df, n.trees = 1000)
train_errors[i] = mean((train.df$log_salary - train_pred)^2)
test_errors[i] = mean((valid.df$log_salary - test_pred)^2)
}
ggplot(data.frame(x=lambdas, y=train_errors), aes(x=x, y=y)) + xlab("Shrinkage") + ylab("Train MSE") + geom_point()
ggplot(data.frame(x=lambdas, y=test_errors), aes(x=x, y=y)) + xlab("Shrinkage") + ylab("Test MSE") + geom_point()
boost_best_model <- gbm(log_salary ~ ., data = train.df, distribution = "gaussian",
n.trees = 1000, shrinkage = lambdas[which.min(test_errors)])
summary(boost_best_model)
bagging <- randomForest(log_salary~., data = train.df, importance = T, mtry = 19)
bagging
bagging_pred <- predict(bagging, valid.df)
mean((bagging_pred-valid.df$log_salary)^2)
lm_Hitter <- lm(log_salary ~ ., data = train.df)
lm_pred <- predict(lm_Hitter, valid.df)
mean((valid.df$log_salary - lm_pred)^2)
bagging <- randomForest(log_salary~., data = train.df, importance = T, mtry = 19)
bagging_pred <- predict(bagging, valid.df)
mean((bagging_pred-valid.df$log_salary)^2)
lambdas
pacman::p_load(ISLR, MASS, rpart, rpart.plot, caret,
randomForest, gbm, tree, ggplot2, dplyr, caTools, leaps, stats, moments )
head(Hitters)
hitters.na.out <- na.omit(Hitters)
str(Hitters)
str(hitters.na.out)
#library('dplyr'), library('moments')
par(mfrow = c(1,2))
hist(hitters.na.out$Salary)
hitters_new<-hitters.na.out%>%mutate(log_salary=log(Salary))
log_transformation<-hist(log(hitters.na.out$Salary))
skewness(hitters_new$log_salary)
# library('ggplot2'), library('dplyr')
hitters.na.out<-hitters.na.out%>%mutate(log_salary=log(Salary))
hitters.na.out<-hitters.na.out[,-c(19)]
ggplot(hitters.na.out,aes(y=Hits,x=Years,col=log_salary))+geom_point()
#library(leaps), library(stats)
models <- regsubsets(log_salary~., data = hitters.na.out, nvmax = 19)
res.sum <- summary(models)
res.sum$bic
num<-data.frame(BIC = which.min(res.sum$bic))#results is 3 -159.2777
num
res.sum$which
#library(caTools)
set.seed(42)
sample = sample.split(hitters.na.out$log_salary, SplitRatio = .80)
train.df= subset(hitters.na.out, sample == TRUE)
valid.df= subset(hitters.na.out, sample == FALSE)
#library(MASS),library(rpart),library(rpart.plot)
tree.baseball <- rpart(log_salary ~ Hits + Years,data =train.df)
#summary(tree.baseball)
rpart.plot(tree.baseball)
tree.baseball$variable.importance
#Regression Tree
tree.baseball_1<-rpart(log_salary~.,data=train.df)
#summary(tree.baseball_1)
rpart.plot(tree.baseball_1)
tree.baseball_1$variable.importance
lambdas <- c(c(), seq(0.002, 0.01, by=0.001))
lambdas <- c(lambdas, seq(0.02, 0.1, by=0.01))
lambdas <- c(lambdas, seq(0.2, 1, by=0.1))
lambdas = 4^lambdas
length.lambdas <- length(lambdas)
train_errors <- rep(NA, length.lambdas)
test_errors <- rep(NA, length.lambdas)
for (i in 1:length.lambdas) {
boost.hitters <- gbm(log_salary~ ., data = train.df, distribution = "gaussian",
n.trees = 1000, shrinkage = lambdas[i])
train_pred <- predict(boost.hitters, train.df, n.trees = 1000)
test_pred <- predict(boost.hitters, valid.df, n.trees = 1000)
train_errors[i] = mean((train.df$log_salary - train_pred)^2)
test_errors[i] = mean((valid.df$log_salary - test_pred)^2)
}
ggplot(data.frame(x=lambdas, y=train_errors), aes(x=x, y=y)) + xlab("Shrinkage") + ylab("Train MSE") + geom_point()
lambdas
powers = seq(-10, -0.2, by = 0.1)
lambdas = 10^powers
lambdas
powers
pacman::p_load(ISLR, MASS, rpart, rpart.plot, caret,
randomForest, gbm, tree, ggplot2, dplyr, caTools, leaps, stats, moments )
head(Hitters)
hitters.na.out <- na.omit(Hitters)
str(Hitters)
str(hitters.na.out)
#library('dplyr'), library('moments')
par(mfrow = c(1,2))
hist(hitters.na.out$Salary)
hitters_new<-hitters.na.out%>%mutate(log_salary=log(Salary))
log_transformation<-hist(log(hitters.na.out$Salary))
skewness(hitters_new$log_salary)
# library('ggplot2'), library('dplyr')
hitters.na.out<-hitters.na.out%>%mutate(log_salary=log(Salary))
hitters.na.out<-hitters.na.out[,-c(19)]
ggplot(hitters.na.out,aes(y=Hits,x=Years,col=log_salary))+geom_point()
#library(leaps), library(stats)
models <- regsubsets(log_salary~., data = hitters.na.out, nvmax = 19)
res.sum <- summary(models)
res.sum$bic
num<-data.frame(BIC = which.min(res.sum$bic))#results is 3 -159.2777
num
res.sum$which
#library(caTools)
set.seed(42)
sample = sample.split(hitters.na.out$log_salary, SplitRatio = .80)
train.df= subset(hitters.na.out, sample == TRUE)
valid.df= subset(hitters.na.out, sample == FALSE)
#library(MASS),library(rpart),library(rpart.plot)
tree.baseball <- rpart(log_salary ~ Hits + Years,data =train.df)
#summary(tree.baseball)
rpart.plot(tree.baseball)
tree.baseball$variable.importance
#Regression Tree
tree.baseball_1<-rpart(log_salary~.,data=train.df)
#summary(tree.baseball_1)
rpart.plot(tree.baseball_1)
tree.baseball_1$variable.importance
#lambdas <- c(c(), seq(0.002, 0.01, by=0.001))
#lambdas <- c(lambdas, seq(0.02, 0.1, by=0.01))
#lambdas <- c(lambdas, seq(0.2, 1, by=0.1))
set.seed(42)
p = seq(-10, -0.2, by = 0.1)
lambdas = 10^p
length.lambdas <- length(lambdas)
train_errors <- rep(NA, length.lambdas)
test_errors <- rep(NA, length.lambdas)
for (i in 1:length.lambdas) {
boost.hitters <- gbm(log_salary~ ., data = train.df, distribution = "gaussian",
n.trees = 1000, shrinkage = lambdas[i])
train_pred <- predict(boost.hitters, train.df, n.trees = 1000)
test_pred <- predict(boost.hitters, valid.df, n.trees = 1000)
train_errors[i] = mean((train.df$log_salary - train_pred)^2)
test_errors[i] = mean((valid.df$log_salary - test_pred)^2)
}
ggplot(data.frame(x=lambdas, y=train_errors), aes(x=x, y=y)) + xlab("Shrinkage") + ylab("Train MSE") + geom_point()
ggplot(data.frame(x=lambdas, y=test_errors), aes(x=x, y=y)) + xlab("Shrinkage") + ylab("Test MSE") + geom_point()
boost_best_model <- gbm(log_salary ~ ., data = train.df, distribution = "gaussian",
n.trees = 1000, shrinkage = lambdas[which.min(test_errors)])
summary(boost_best_model)
lm_Hitter <- lm(log_salary ~ ., data = train.df)
lm_pred <- predict(lm_Hitter, valid.df)
mean((valid.df$log_salary - lm_pred)^2)
bagging <- randomForest(log_salary~., data = train.df, importance = T, mtry = 19)
bagging_pred <- predict(bagging, valid.df)
mean((bagging_pred-valid.df$log_salary)^2)
boost_best_model <- gbm(log_salary ~ ., data = train.df, distribution = "gaussian",
n.trees = 1000, shrinkage = lambdas[which.min(test_errors)])
head(summary(boost_best_model))
pacman::p_load(ISLR, MASS, rpart, rpart.plot, caret,
randomForest, gbm, tree, ggplot2, dplyr, caTools, leaps, stats, moments )
head(Hitters)
hitters.na.out <- na.omit(Hitters)
str(Hitters)
str(hitters.na.out)
#library('dplyr'), library('moments')
par(mfrow = c(1,2))
hist(hitters.na.out$Salary)
hitters_new<-hitters.na.out%>%mutate(log_salary=log(Salary))
log_transformation<-hist(log(hitters.na.out$Salary))
skewness(hitters_new$log_salary)
# library('ggplot2'), library('dplyr')
hitters.na.out<-hitters.na.out%>%mutate(log_salary=log(Salary))
hitters.na.out<-hitters.na.out[,-c(19)]
ggplot(hitters.na.out,aes(y=Hits,x=Years,col=log_salary))+geom_point()
#library(leaps), library(stats)
models <- regsubsets(log_salary~., data = hitters.na.out, nvmax = 19)
res.sum <- summary(models)
res.sum$bic
num<-data.frame(BIC = which.min(res.sum$bic))#results is 3 -159.2777
num
res.sum$which
#library(caTools)
set.seed(42)
sample = sample.split(hitters.na.out$log_salary, SplitRatio = .80)
train.df= subset(hitters.na.out, sample == TRUE)
valid.df= subset(hitters.na.out, sample == FALSE)
#library(MASS),library(rpart),library(rpart.plot)
tree.baseball <- rpart(log_salary ~ Hits + Years,data =train.df)
#summary(tree.baseball)
rpart.plot(tree.baseball)
tree.baseball$variable.importance
#Regression Tree
tree.baseball_1<-rpart(log_salary~.,data=train.df)
#summary(tree.baseball_1)
rpart.plot(tree.baseball_1)
tree.baseball_1$variable.importance
#lambdas <- c(c(), seq(0.002, 0.01, by=0.001))
#lambdas <- c(lambdas, seq(0.02, 0.1, by=0.01))
#lambdas <- c(lambdas, seq(0.2, 1, by=0.1))
set.seed(42)
p = seq(-5, -0.2, by = 0.1)
lambdas = 10^p
length.lambdas <- length(lambdas)
train_errors <- rep(NA, length.lambdas)
test_errors <- rep(NA, length.lambdas)
for (i in 1:length.lambdas) {
boost.hitters <- gbm(log_salary~ ., data = train.df, distribution = "gaussian",
n.trees = 1000, shrinkage = lambdas[i])
train_pred <- predict(boost.hitters, train.df, n.trees = 1000)
test_pred <- predict(boost.hitters, valid.df, n.trees = 1000)
train_errors[i] = mean((train.df$log_salary - train_pred)^2)
test_errors[i] = mean((valid.df$log_salary - test_pred)^2)
}
ggplot(data.frame(x=lambdas, y=train_errors), aes(x=x, y=y)) + xlab("Shrinkage") + ylab("Train MSE") + geom_point()
ggplot(data.frame(x=lambdas, y=test_errors), aes(x=x, y=y)) + xlab("Shrinkage") + ylab("Test MSE") + geom_point()
boost_best_model <- gbm(log_salary ~ ., data = train.df, distribution = "gaussian",
n.trees = 1000, shrinkage = lambdas[which.min(test_errors)])
head(summary(boost_best_model))
lm_Hitter <- lm(log_salary ~ ., data = train.df)
lm_pred <- predict(lm_Hitter, valid.df)
mean((valid.df$log_salary - lm_pred)^2)
bagging <- randomForest(log_salary~., data = train.df, importance = T, mtry = 19)
bagging_pred <- predict(bagging, valid.df)
mean((bagging_pred-valid.df$log_salary)^2)
lm_Hitter <- lm(log_salary ~ ., data = train.df)
lm_pred <- predict(lm_Hitter, valid.df)
mean((valid.df$log_salary - lm_pred)^2)
bagging <- randomForest(log_salary~., data = train.df, importance = T, mtry = 19)
bagging_pred <- predict(bagging, valid.df)
mean((bagging_pred-valid.df$log_salary)^2)
lm_Hitter <- lm(log_salary ~ ., data = train.df)
lm_pred <- predict(lm_Hitter, valid.df)
mean((valid.df$log_salary - lm_pred)^2)
bagging <- randomForest(log_salary~., data = train.df, importance = T, mtry = 19)
bagging_pred <- predict(bagging, valid.df)
mean((bagging_pred-valid.df$log_salary)^2)
set.seed(42)
lm_Hitter <- lm(log_salary ~ ., data = train.df)
lm_pred <- predict(lm_Hitter, valid.df)
mean((valid.df$log_salary - lm_pred)^2)
bagging <- randomForest(log_salary~., data = train.df, importance = T, mtry = 19)
bagging_pred <- predict(bagging, valid.df)
mean((bagging_pred-valid.df$log_salary)^2)
set.seed(42)
lm_Hitter <- lm(log_salary ~ ., data = train.df)
lm_pred <- predict(lm_Hitter, valid.df)
mean((valid.df$log_salary - lm_pred)^2)
bagging <- randomForest(log_salary~., data = train.df, importance = T, mtry = 19)
bagging_pred <- predict(bagging, valid.df)
mean((bagging_pred-valid.df$log_salary)^2)
set.seed(42)
lm_Hitter <- lm(log_salary ~ ., data = train.df)
lm_pred <- predict(lm_Hitter, valid.df)
mean((valid.df$log_salary - lm_pred)^2)
bagging <- randomForest(log_salary~., data = train.df, importance = T, mtry = 19)
bagging_pred <- predict(bagging, valid.df)
mean((bagging_pred-valid.df$log_salary)^2)
#library('dplyr'), library('moments')
par(mfrow = c(1,2))
hist(hitters.na.out$Salary,main="Histogram of Salary")
#library('dplyr'), library('moments')
par(mfrow = c(1,2))
hist(hitters.na.out$Salary,,main="Histogram of Salary")
#library('dplyr'), library('moments')
par(mfrow = c(1,2))
hist(hitters.na.out$Salary,hitters.na.out$.,main="Histogram of Salary")
#library('dplyr'), library('moments')
par(mfrow = c(1,2))
hist(hitters.na.out$Salary,)+main="Histogram of Salary"
#library('dplyr'), library('moments')
par(mfrow = c(1,2))
hist(hitters.na.out$Salary)+main="Histogram of Salary"
#library('dplyr'), library('moments')
par(mfrow = c(1,2))
hist(hitters.na.out$Salary)
pacman::p_load(ISLR, MASS, rpart, rpart.plot, caret,
randomForest, gbm, tree, ggplot2, dplyr, caTools, leaps, stats, moments )
pacman::p_load(ISLR, MASS, rpart, rpart.plot, caret,
randomForest, gbm, tree, ggplot2, dplyr, caTools, leaps, stats, moments )
head(Hitters)
hitters.na.out <- na.omit(Hitters)
str(Hitters)
str(hitters.na.out)
#library('dplyr'), library('moments')
par(mfrow = c(1,2))
hist(hitters.na.out$Salary)
hitters_new<-hitters.na.out%>%mutate(log_salary=log(Salary))
log_transformation<-hist(log(hitters.na.out$Salary))
skewness(hitters_new$log_salary)
# library('ggplot2'), library('dplyr')
hitters.na.out<-hitters.na.out%>%mutate(log_salary=log(Salary))
hitters.na.out<-hitters.na.out[,-c(19)]
ggplot(hitters.na.out,aes(y=Hits,x=Years,col=log_salary))+geom_point()
#library(leaps), library(stats)
models <- regsubsets(log_salary~., data = hitters.na.out, nvmax = 19)
res.sum <- summary(models)
res.sum$bic
num<-data.frame(BIC = which.min(res.sum$bic))#results is 3 -159.2777
num
res.sum$which[1:8,]
#library(caTools)
set.seed(42)
sample = sample.split(hitters.na.out$log_salary, SplitRatio = .80)
train.df= subset(hitters.na.out, sample == TRUE)
valid.df= subset(hitters.na.out, sample == FALSE)
#library(MASS),library(rpart),library(rpart.plot)
tree.baseball <- rpart(log_salary ~ Hits + Years,data =train.df)
#summary(tree.baseball)
rpart.plot(tree.baseball)
tree.baseball$variable.importance
#Regression Tree
tree.baseball_1<-rpart(log_salary~.,data=train.df)
#summary(tree.baseball_1)
rpart.plot(tree.baseball_1)
tree.baseball_1$variable.importance
#lambdas <- c(c(), seq(0.002, 0.01, by=0.001))
#lambdas <- c(lambdas, seq(0.02, 0.1, by=0.01))
#lambdas <- c(lambdas, seq(0.2, 1, by=0.1))
set.seed(42)
p = seq(-5, -0.2, by = 0.1)
lambdas = 10^p
length.lambdas <- length(lambdas)
train_errors <- rep(NA, length.lambdas)
test_errors <- rep(NA, length.lambdas)
for (i in 1:length.lambdas) {
boost.hitters <- gbm(log_salary~ ., data = train.df, distribution = "gaussian",
n.trees = 1000, shrinkage = lambdas[i])
train_pred <- predict(boost.hitters, train.df, n.trees = 1000)
test_pred <- predict(boost.hitters, valid.df, n.trees = 1000)
train_errors[i] = mean((train.df$log_salary - train_pred)^2)
test_errors[i] = mean((valid.df$log_salary - test_pred)^2)
}
ggplot(data.frame(x=lambdas, y=train_errors), aes(x=x, y=y)) + xlab("Shrinkage") + ylab("Train MSE") + geom_point()
ggplot(data.frame(x=lambdas, y=test_errors), aes(x=x, y=y)) + xlab("Shrinkage") + ylab("Test MSE") + geom_point()
boost_best_model <- gbm(log_salary ~ ., data = train.df, distribution = "gaussian",
n.trees = 1000, shrinkage = lambdas[which.min(test_errors)])
head(summary(boost_best_model))
set.seed(42)
lm_Hitter <- lm(log_salary ~ ., data = train.df)
lm_pred <- predict(lm_Hitter, valid.df)
mean((valid.df$log_salary - lm_pred)^2)
bagging <- randomForest(log_salary~., data = train.df, importance = T, mtry = 19)
bagging_pred <- predict(bagging, valid.df)
mean((bagging_pred-valid.df$log_salary)^2)
if(!require("pacman")) install.packages("pacman")
pacman::p_load(ISLR, MASS, rpart, rpart.plot, caret,
randomForest, gbm, tree, ggplot2, dplyr, caTools, leaps, stats, moments )
#head(Hitters)
hitters.na.out <- na.omit(Hitters)
str(Hitters)
str(hitters.na.out)
#library('dplyr'), library('moments')
par(mfrow = c(1,2))
hist(hitters.na.out$Salary)
hitters_new<-hitters.na.out%>%mutate(log_salary=log(Salary))
log_transformation<-hist(log(hitters.na.out$Salary))
skewness(hitters_new$log_salary)
# library('ggplot2'), library('dplyr')
hitters.na.out<-hitters.na.out%>%mutate(log_salary=log(Salary))
hitters.na.out<-hitters.na.out[,-c(19)]
ggplot(hitters.na.out,aes(y=Hits,x=Years,col=log_salary))+geom_point()
#library(leaps), library(stats)
models <- regsubsets(log_salary~., data = hitters.na.out, nvmax = 19)
res.sum <- summary(models)
res.sum$bic
num<-data.frame(BIC = which.min(res.sum$bic))
num #results is 3 -159.2777
res.sum$which[1:5,]
#library(caTools)
set.seed(42)
sample = sample.split(hitters.na.out$log_salary, SplitRatio = .80)
train.df= subset(hitters.na.out, sample == TRUE)
valid.df= subset(hitters.na.out, sample == FALSE)
#library(MASS),library(rpart),library(rpart.plot)
tree.baseball <- rpart(log_salary ~ Hits + Years,data =train.df)
#summary(tree.baseball)
rpart.plot(tree.baseball)
tree.baseball$variable.importance
#Regression Tree
tree.baseball_1<-rpart(log_salary~.,data=train.df)
#summary(tree.baseball_1)
rpart.plot(tree.baseball_1)
tree.baseball_1$variable.importance
#lambdas <- c(c(), seq(0.002, 0.01, by=0.001))
#lambdas <- c(lambdas, seq(0.02, 0.1, by=0.01))
#lambdas <- c(lambdas, seq(0.2, 1, by=0.1))
set.seed(42)
p = seq(-5, -0.2, by = 0.1)
lambdas = 10^p
length.lambdas <- length(lambdas)
train_errors <- rep(NA, length.lambdas)
test_errors <- rep(NA, length.lambdas)
for (i in 1:length.lambdas) {
boost.hitters <- gbm(log_salary~ ., data = train.df, distribution = "gaussian",
n.trees = 1000, shrinkage = lambdas[i])
train_pred <- predict(boost.hitters, train.df, n.trees = 1000)
test_pred <- predict(boost.hitters, valid.df, n.trees = 1000)
train_errors[i] = mean((train.df$log_salary - train_pred)^2)
test_errors[i] = mean((valid.df$log_salary - test_pred)^2)
}
ggplot(data.frame(x=lambdas, y=train_errors), aes(x=x, y=y)) + xlab("Shrinkage") + ylab("Train MSE") + geom_point()
ggplot(data.frame(x=lambdas, y=test_errors), aes(x=x, y=y)) + xlab("Shrinkage") + ylab("Test MSE") + geom_point()
boost_best_model <- gbm(log_salary ~ ., data = train.df, distribution = "gaussian",
n.trees = 1000, shrinkage = lambdas[which.min(test_errors)])
head(summary(boost_best_model))
set.seed(42)
lm_Hitter <- lm(log_salary ~ ., data = train.df)
lm_pred <- predict(lm_Hitter, valid.df)
mean((valid.df$log_salary - lm_pred)^2)
bagging <- randomForest(log_salary~., data = train.df, importance = T, mtry = 19)
bagging_pred <- predict(bagging, valid.df)
mean((bagging_pred-valid.df$log_salary)^2)
#head(Hitters)
Hitters
#head(Hitters)
Hitters
hitters.na.out <- na.omit(Hitters$Salary)
str(Hitters)
str(hitters.na.out)
#head(Hitters)
Hitters
hitters.na.out <- na.omit(Hitters$Salary)
str(Hitters)
str(hitters.na.out)
str(hitters.na.out)
#head(Hitters)
Hitters
hitters.na.out <- na.omit(Hitters)
str(Hitters)
str(hitters.na.out)
pacman::p_load(ISLR, MASS, rpart, rpart.plot, caret,
randomForest, gbm, tree, ggplot2, dplyr, caTools, leaps, stats, moments )
#head(Hitters)
hitters.na.out <- Hitters[!is.na(Hitters$Salary),]
str(Hitters)
str(hitters.na.out)
#library('dplyr'), library('moments')
par(mfrow = c(1,2))
hist(hitters.na.out$Salary)
hitters_new<-hitters.na.out%>%mutate(log_salary=log(Salary))
log_transformation<-hist(log(hitters.na.out$Salary))
skewness(hitters_new$log_salary)
# library('ggplot2'), library('dplyr')
hitters.na.out<-hitters.na.out%>%mutate(log_salary=log(Salary))
hitters.na.out<-hitters.na.out[,-c(19)]
ggplot(hitters.na.out,aes(y=Hits,x=Years,col=log_salary))+geom_point()
#library(leaps), library(stats)
models <- regsubsets(log_salary~., data = hitters.na.out, nvmax = 19)
res.sum <- summary(models)
res.sum$bic
num<-data.frame(BIC = which.min(res.sum$bic))
num #results is 3 -159.2777
res.sum$which[1:5,]
#library(caTools)
set.seed(42)
sample = sample.split(hitters.na.out$log_salary, SplitRatio = .80)
train.df= subset(hitters.na.out, sample == TRUE)
valid.df= subset(hitters.na.out, sample == FALSE)
#library(MASS),library(rpart),library(rpart.plot)
tree.baseball <- rpart(log_salary ~ Hits + Years,data =train.df)
#summary(tree.baseball)
rpart.plot(tree.baseball)
tree.baseball$variable.importance
#Regression Tree
tree.baseball_1<-rpart(log_salary~.,data=train.df)
#summary(tree.baseball_1)
rpart.plot(tree.baseball_1)
tree.baseball_1$variable.importance
#lambdas <- c(c(), seq(0.002, 0.01, by=0.001))
#lambdas <- c(lambdas, seq(0.02, 0.1, by=0.01))
#lambdas <- c(lambdas, seq(0.2, 1, by=0.1))
set.seed(42)
p = seq(-5, -0.2, by = 0.1)
lambdas = 10^p
length.lambdas <- length(lambdas)
train_errors <- rep(NA, length.lambdas)
test_errors <- rep(NA, length.lambdas)
for (i in 1:length.lambdas) {
boost.hitters <- gbm(log_salary~ ., data = train.df, distribution = "gaussian",
n.trees = 1000, shrinkage = lambdas[i])
train_pred <- predict(boost.hitters, train.df, n.trees = 1000)
test_pred <- predict(boost.hitters, valid.df, n.trees = 1000)
train_errors[i] = mean((train.df$log_salary - train_pred)^2)
test_errors[i] = mean((valid.df$log_salary - test_pred)^2)
}
ggplot(data.frame(x=lambdas, y=train_errors), aes(x=x, y=y)) + xlab("Shrinkage") + ylab("Train MSE") + geom_point()
ggplot(data.frame(x=lambdas, y=test_errors), aes(x=x, y=y)) + xlab("Shrinkage") + ylab("Test MSE") + geom_point()
boost_best_model <- gbm(log_salary ~ ., data = train.df, distribution = "gaussian",
n.trees = 1000, shrinkage = lambdas[which.min(test_errors)])
head(summary(boost_best_model))
set.seed(42)
lm_Hitter <- lm(log_salary ~ ., data = train.df)
lm_pred <- predict(lm_Hitter, valid.df)
mean((valid.df$log_salary - lm_pred)^2)
bagging <- randomForest(log_salary~., data = train.df, importance = T, mtry = 19)
bagging_pred <- predict(bagging, valid.df)
mean((bagging_pred-valid.df$log_salary)^2)
